#+STARTUP: showeverything logdone
#+options: num:nil



(Conversational: I am wondering about hooking up/expanding this: [[file:grammar_for_hcode.org][grammar_for_hcode]] with this: [[file:What_can_the_expression_parser_do.org][What_can_the_expression_parser_do]]. And then somehow those need to be supplemented or cemented to grammatical productions yielding logical statements that can be proved true or false. I think it is clear that something very Ghilbert-like could be suitable for representing theorems and their proofs -- perhaps your sexp's could be changed to meet Ghilbert's requirements, or you could create a "Hcodebert" variant of Ghilbert that uses your format. 

Another item of interest involves: [[file:designing_proof_structures_for_hcode.org][designing_proof_structures_for_hcode]] -- the Metamath/Ghilbert proofs construct formulas by supplying the names/labels of justifications in the correct order, which is a reversal of the approach shown here: [[file:Example_of_structured_proof.org][Example_of_structured_proof]], which is a good way, as demonstrated here: [[file:Proof_of_distributivity_of_product_over_union_with_justifications.org][Proof_of_distributivity_of_product_over_union_with_justifications]]. If you examine those justifications, such as "distributive law of and over or" there is still a certain amount of informality, yes? So it is necessary to provide the name of an actual theorem, axiom or definition which can be checked. Metamath.pdf has stuff about "condensed detachment" which is dense :)

Regardless of which proof format(s) and parsed expression format(s) are selected, a database of theorems, axioms and definitions must be available for proof verification. A classic bootstrap problem for HDM? Planning to start from the very beginning? You can be logically "agnostic" but isn't HDM going to actually use the classic propositional logic, predicate calculus and ZFC set theory?) --[[file:ocat.org][ocat]]

Here are some replies to the points you raise:

: And then somehow those need to be supplemented or cemented to grammatical productions yielding logical statements that can be proved true or false.

This sounds a bit like my original ideas which I have since moved away from.  Originally, I envisioned a two step process in which  mathematical text would be translated into s-expressions and these would subsequently be translated into logical propositions whose soundness could be checked by some other program.  However, as I now see it, this is not necessary and perhaps inefficient.  I am finding that it is as easy to check validity of s-expressions directly as to first translate them into logical propositions and then check validity.  Moreover, sometimes translating mathematical expressions can lead to lenghty messes so it would be best to avoid such translations.

It is easy enough to write logical propositions as s-expressions --- it basically amounts to Lukasiewicz's Polish notation with a liberal sprinkling of parentheses.  Therefore, translating s-expressions into logical propositions can be seen as an exercise in translating s-expressions into a specific type of s-expression.  

: perhaps your sexp's could be changed to meet Ghilbert's requirements, or you could create a "Hcodebert" variant of Ghilbert that uses your format

Personally, I would think that, when the issue comes up, the best thing to do would be to write programs which translates between the s-expressions and Ghilbert requirements.  One of the things I am all for is having the computer translate between notations so that people and computers can see mathematics in the formats which they find most comfortable.

For the purposes of what I am trying to do, the s-expression format is the most natural way of expressing mathematics because I am making extensive use of LISP.  At a superficial level, it is convenient because LISP is based on s-expressions, so there is no overhead in having to translate or interpret a different format.  At a more profound level, I think of LISP not merely as a programming language, but as a mathematical or metamathematical framework.  This is not too wild given that LISP is basically a particular implementation of the lamda calculus.  A lot of what Joe and I did was to think of mathematics as LISP in the hope that this would make it easier to teach it to a computer.

: there is still a certain amount of informality, yes?

Of course there is.  The point of what we were doing was to examine mathematics as it appears in math books --- in fact this particular example was taken from an algebra text.  Therefore, there was still a certain amount of informality in the example.

The reason for doing this has to do with the approach we are taking.  Our grand goal is to have a computer be able to understand mathematics as it appears in math books.  Since this involves *lots* of informality, that means that, at some point, we will have to learn how to deal with this informality.  We expect that doing this will involve AI techniques such as heuristic searches.  To be sure, we haven't gotten to the point of working on this yet but, since we know that this is the direction we are headed in, we are preparing by considering mathematics as it is usually written by mathematicians alongside the "cleaned up" purely formal version of the same so that we can understand exactly what informality is and so that we can keep our formal manipulations as close as possible to what mathematicians typically do informally and hope that this policy will make it easier for us to learn how to teach the computer to understand informal proofs.

As you note, the usual approach one sees in mathematical literature is rather the opposite of the approach that is natural when writing a logic verification program.  In the former case, the emphasis is on the statements and the justifications are usually omitted or only given in part since the typical reader can easily provide the justifications if necessary and even rewrite the proof purely formally in some formal system if needs be.  In the latter case, say in your program, one can focus on the justifications and let the program construct formulas.  The challenge is to make the two ends meet.  To do this, we are working on the problem from both ends, which is why you see us going backwards and forwards.

In the real world where proofs come in various levels of informality, it seems that verifying proofs will be a like automatic theorem proving in that one may need to provide missing steps and justifications in the proof.

:  a database of theorems, axioms and definitions must be available for proof verification.

The database we plan to use is the mathematical literature.  To do this, technological and sociological objectives need to be overcome.  

To begin, the mathematical literature need to be accessible in a computer-readable format.  There is no technical reason why one couldn't have a copy of all the mathematical literature ever written to date sitting in a disk on one's desktop --- the new literature typically is written in !TeX, the older literature can be (and is being) retro-digitized, and disks with terabyte capacity not only exist, but are affordable by the average person.  Rather, the barrier is sociological --- copyright law forbids one from writing such a disk unless one has the permission of the owners of all the copyrights to the various math books and articles.  Typically, gettting permission involves paying a fee, and these fees would be orders of magnitude greater than the actual costs involved in preparing such a disk.  To deal with this issue, we are actively involved in promoting the creation of free math.  If Planet Math and like projects continue and flourish, we can expect that there will be a corpus of free mathematical
literature which contains almost all existing mathematical knowledge.

Even if one had this hypothetical disk in one pocket, ther would still be the issue that, while it makes sense to humans, it nees to be translated in order to be of use to computers.  This is where we encounter the technological barrier --- how do we do this translation?  

The easy (I use the word in a relative sense) part is to translate the mathematical expressions which appear in the literature.  While far from trivial, translating mathematical expressions as written in !TeX to s-expressions or Ghilbert or some such format seems plausible.  We have already had success with simpler expressions, with some more elbow grease (as opposed to new insights) we could certainly extend the system so that it could deal with most simple and moderately complicated expressions.  

The hard part is natural language.  However, there is a silver lining in this cloud.  A good and important part of what one finds in math books is language which is meant to express meaning which could be expressed symbolically (in fact one often finds words and symbols intermixed).  Dealing with this sort of language should be easier that the general problem of understanding natural language for two reasons.  First, this language uses a restricted vocabulary and many peculiar idioms (such as "if and only if").  Second, unlike in general, we can exhibit a precise formulation of the meaning of such a sentence as a symbolic expression.  

: You can be logically "agnostic" but isn't HDM going to actually use the classic propositional logic

Of course it will.  Agnosticism simply means that it doesn't have to use classic logic but, in actual practise, it typically will.  Personally, I think of logic agnosticism more as a nice design goal than as a pressing issue.  I would be more than happy with a system which would be able to deal with math as found in math books even if it was limited to classical logic.  Also, for me the main value of keeping this ideal of logic agnosticism in mind is that it helps me to think of things in an appropriate level of generality.

--[[file:rspuzio.org][rspuzio]] 18 September 2005

(More Conversation: 

1. I suggest having a look at the Ghilbert/Metamath comparison, parts 1 and 2 (so far): [[file:GhilbertVsMetamathPart1.org][GhilbertVsMetamathPart1]]. One reason I made the effort is to provide an assist here. Ghilbert *uses* sexp's but the punctuation is slightly different. It's almost ready to go, and it can deal with alternative axiomatizations. 

2. Saying "The database we plan to use is the mathematical literature" seems to just defer the inevitable. A pile of books, or a huge set of files even, is not a "database". Even a "smart" AI program will need a store of knowledge that represents what it has learned over time. You cannot be thinking that to verify a differential equations proof your AI will first re-read the book on propositional logic to learn how to reason.

3. Re: "AI techniques such as heuristic searches" -- I recommend a pilot project for "proof of concept". Be prepared to do it once and then start over again, because the complexity is immense and there are so many unknowns. In my humble little mmj2 project, which simply adds grammatical parsing to Metamath proof verification, it took 3 tries to get the parser working at a reasonable level. My second try involved the Bottom Up Parsing algorithm, which is beautifully simple and worked great until about 1/3 of the way through set.mm, when it hit "supeu". After letting the code run for 2 hours I pulled the plug and started over again, this time with the Earley Parse algorithm (which, by the way, handles ambiguous grammars). It turns out that dumb AI is just algorithms, and you might have a logically valid algorithm that is impractical in the Platonic Universe of math. ) -- [[file:ocat.org][ocat]] 18 September 2005

Quick reply to these points: (1) We'll definitely be looking into this stuff, when the time is right -- it isn't quite right for me yet, because I'm absorbed with something else right now; (2) and (3) are essentially what I'm absorbed with.  --[[file:jcorneli.org][jcorneli]]

Ray's replies:

 1. I will have a look at what you put up and let you know what I think when I get a chance.  I am glad that you took an interest in Asteroid and are contributing.  I didn't get a chance to welcome you aboard earlier because I was too busy to contribute last month so I hope a belated welcome is better than no welcome at all --- welcome aboard the Asteroid, Ocat; glad to have you here!

 1. The strategy in delaying the inevitable is to delay it long enough that the computer will be able to do the brunt of the work of preparing this database from the information contained in the pile of books and files.  What this means in the immediate future is that, rather than putting my time into preparing the database by hand, I will instead focus my effort on figuring out how to have a computer understand mathematical text well enough to represent it in its internal format.  Since I have focussed my attention towards checking proofs, I haven't done much in this direction recently but, when we looked into this matter a few months back, Joe came out with an expression parser which could handle translate simple mathematical and logical expressions from !TeX to s-expressions and vice versa.  With more elbow grease, it should be possible to get this system to deal with a large class of expressions so that, for instance, it would be possible to have the computer translate the expressions found in an
elementary analysis book.

 1. I should start by mentioning that I haven't been putting much serious effort into the business of supplying missing steps in proofs since I have been focussing on figuring out the representation language and on verifying proofs in which nothing has been left out.  I agree with your suggestion and your reminder that algorithms can be extremely inefficient.  In reply let me offer another suggestion (which I plan to take when the time comes) and an observation.

## To teach a computer to deal with informality in proofs, I would want to understand informality in proofs better.  In particular, there is the aspect of leaving out statements and justifications.  If you ask a mathematician why a certain statement or justification was left out, their usual answer would be "because it was obvious".  I propose to provide more detail and figure out exactly what this term "obvious" means by studting actual proofs.  My plan is to first take a bunch of proofs as they appear in math books, then add in whatever missing information is needed to make the proof detailed enough that the computer could check its validity.  Then I can go back and look at this to see what generalizations I can make about what was omitted and try to turn this into some heuristcs which could be used to guide the computer.  My suspicion is that good heuristics might be as important as good algorithms here but, until I get around to studying this aspect of the project, this is just my guess.

##  In this sort of situation, even an impractacally slow algorithm can still be valuable.  After all, in AI the first question often is "Can  a computer do this ?" so even an impractically sluggish algorithm may be worthy of note as answering this question.  Of course, that doesn't mean one should be content with the slow algorithm; rather it can serve as a starting point for looking for more practical solutions.

--[[file:rspuzio.org][rspuzio]] 19 September 2005

([[file:ocat.org][ocat]] --> Thanks for the kind words. I think HDM is an interesting project and potentially very valuable. A point to consider is that since the average human cannot assimilate a basic mathematical textbook, the goal of having a computer read arbitrary math texts is very ambitious. Another challenge that restricts even the Gutenberg project is the reliance upon diagrams to supplement the verbiage (how to process "Elements"?) There is even the question of whether or not true communication actually takes place when mathematicians discuss math using English :-0) 

I have another recommendation (oh boy). I don't know if everyone here has actually tried using Metamath.exe, but for HDM'ers, edit a copy of set.mm and delete everything after the first 50 theorems. Then delete the proofs of those theorems (replace with "?"), reinvent the proofs (no cheating!) of the first 50 theorems starting from the preceding axioms, and enter them into Metamath. That will give you a solid idea of what to expect of a computer system that has no knowledge of math or logic, as well as an alternate view of things from a bug's eye-level. I suspect you will be horrified and amazed :)
